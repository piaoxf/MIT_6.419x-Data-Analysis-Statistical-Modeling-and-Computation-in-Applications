## Objectives

In this lecture and series of exercises you will learn:

- What it means for a function to be convex, and why convex functions are important in optimization.

- Critical points and saddle points in multi-dimensional optimization.

- Definitions for convexity in multiple dimensions.

- How to create local quadratic approximations to a function.

- The method of gradient descent.

- How to select the step size in gradient descent.

- The stochastic gradient descent algorithm.

After completing this lecture, you will be able to:

- Determine, via three conditions, if a function is convex .

- Minimize a convex optimization problem using Newton's method .

- Use gradient descent to optimize non-convex optimization problems.

- Understand when and how to use stochastic gradient descent .

## Convexity

In this unit, we will

- Define a loss function and how it is used in optimization.

- Discuss the difference between a global and a local minimum of a loss function.

- Determine if a critical point of the loss function is a minium or a maximum.

- Define the convexity property of a function three ways, and show how each definition is related to the others.



