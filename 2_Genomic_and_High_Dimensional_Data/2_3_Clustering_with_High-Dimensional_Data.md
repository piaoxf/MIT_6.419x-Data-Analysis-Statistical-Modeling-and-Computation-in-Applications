## Objectives

At the end of this lecture, you will be able to

- implement the K-means and the alternative K-medoids (or partitioning around medoids) algorithms.

- Inspect an elbow plot to choose the number of clusters for K-means.

- Know the principals behind the expectation-maximization (EM) algorithm as an algorithm to estimate the maximum likelihood estimates for the parameters of a Gaussian Mixture Model (GMM) .

- Perform agglomerative clustering using different dissimilarity measures and read the associated dendrogram .

- Know the principals behind the density-based spatial clustering of applications with noise (DBSCAN) .

- Evaluate cluster assignments using silouette scores and plots .

##  Motivation

Clustering aims to group of data points together so that within the same group, the observations are very similar, and between different groups, the observations are dissimilar, according to some dissimilarity measure.

The data points do not need to come with any labels. Often, clustering is used as a way to decide what and how many class labels are suitable for the data set. For example, we may want to cluster gene expression data into groups that will characterize different cell types in a blood sample. You will perform such analysis as the homework at the end of this modules.

In this lecture, we will discuss in more detail the common clustering methods motivated in the video above, from algorithms for which we need to first decide the number of clusters, such as K-means and the Gaussian mixture models, to more general approaches such as hierarchical clustering and DBSCAN.

## K-Means

**𝐾-Means Loss Function**

K-means starts with a pre-selected number of clusters  𝐾 , and is a clustering algorithm that aims to minimize the within group sums of squares (WGSS) :

 	 WGSS 	 = 	 ∑𝑘=1𝐾∑𝐱(𝑖),𝐱(𝑗)∈𝐶𝑘𝑑(𝐱(𝑖),𝐱(𝑗))2 	 	 
where the  𝑘  indexes the  𝐾  different clusters,  𝐶𝑘  denotes the  𝑘 -th cluster, and  𝑑(𝐱(𝑖),𝐱(𝑗))  is the distance between the data points  𝐱(𝑖)  and  𝐱(𝑗) . The distance is commonly the Euclidean distance.

The  WGSS  measures how dissimilar data points in the same cluster are. To find an algorithm to minimize  WGSS , it helps to first rewrite  WGSS  in terms of the means  𝝁𝑘=∑𝐱(𝑖)∈𝐶𝑘𝐱(𝑖)/𝑛𝑘  of each cluster  𝐶𝑘 :

 	 WGSS 	 = 	 ∑𝑘=1𝐾∑𝐱(𝑖),𝐱(𝑗)∈𝐶𝑘‖‖𝐱(𝑖)−𝐱(𝑗)‖‖2 	 	 
 	 	 = 	 ∑𝑘=1𝐾∑𝐱(𝑖),𝐱(𝑗)∈𝐶𝑘‖‖(𝐱(𝑖)−𝝁𝑘)−(𝐱(𝑗)−𝝁𝑘)‖‖2 	 	 
 	 	 = 	 ∑𝑘=1𝐾∑𝐱(𝑖),𝐱(𝑗)∈𝐶𝑘(‖‖(𝐱(𝑖)−𝝁𝑘)‖‖2+‖‖𝐱(𝑗)−𝝁𝑘)‖‖2−2(𝐱(𝑖)−𝝁𝑘)⋅(𝐱(𝑗)−𝝁𝑘)) 	 	 
 	 	 = 	 ∑𝑘=1𝐾⎛⎝⎜⎜𝑛𝑘∑𝐱(𝑖)∈𝐶𝑘‖‖(𝐱(𝑖)−𝝁𝑘)‖‖2+𝑛𝑘∑𝐱(𝑗)∈𝐶𝑘‖‖𝐱(𝑗)−𝝁𝑘)‖‖2−2⎛⎝⎜⎜∑𝐱(𝑖)∈𝐶𝑘(𝐱(𝑖)−𝝁𝑘)⎞⎠⎟⎟⋅⎛⎝⎜⎜∑𝐱(𝑗)∈𝐶𝑘(𝐱(𝑗)−𝝁𝑘)⎞⎠⎟⎟⎞⎠⎟⎟ 	 	 
 	 	 = 	 2𝑛𝑘∑𝑘=1𝐾∑𝐱(𝑖)∈𝐶𝑘‖‖(𝐱(𝑖)−𝝁𝑘)‖‖2since ∑𝐱(𝑖)∈𝐶𝑘(𝐱(𝑖)−𝝁𝑘)=0. 	 	 

**𝐾-Means Algorithm**

𝐾 -means minimizes the  WGSS  by the following iterative algorithm.

First, initialize the  𝐾  means  {𝝁𝑘}𝑘=1,…𝐾  to random positions.

Then, repeat the two steps below until the algorithm converges:

Cluster assignment: Cluster each point with the closest centroid  𝝁𝑘 . Call the set of all points in this cluster  𝐶𝑘 .

Centroids Update. Update all centroids  𝝁𝑘  to be the average position of all points assigned to  𝐶𝑘  in the step above .

𝐾 -means does not guarantee convergence to the global minimum, and in fact often converges to a local minimum depending on the random initialization of the cluster centroids. To approach the global minimum, conduct multiple runs starting with different randomly selected cluster centroids. This strategy tends to be more effective when the number of clusters is small (e.g.,  <10 ).

**𝐾-medoids**

There are two limitations associated with K-means:

Its results are sensitive to outliers.

The cluster centroids are not necessarily data points.

To alleviate these two issues, we can modify the algorithm to use medoids instead of means. The medoid of a cluster is the data point that is closest to the mean of that cluster.

**Choosing K**

One heuristic method to determine the number of clusters  𝐾  is the “elbow" method. Plot the loss function  WGSS  across different  𝐾  values, and pick the  𝐾  corresponding to the “elbow" of the plot. For example, according to the plot below, the best choice for  𝐾  is 3.

## Gaussian Mixture Models (GMM) and Expectation-Maximization (EM) Algorithm

Clustering using Gaussian mixture model (GMM) generalizes  𝐾 -mean in two ways:

- Cluster assignment is based on the probabilities of the data point being generated by the different clusters.

- The shape of the clusters can be elliptical rather than only spherical.

**Estimating the Parameters of Gaussian Mixture Models (GMM) using the Expectation-Maximization (EM) Algorithm**

Consider the Gaussian mixture model of  𝐾  Gaussians:

 	 𝐏(𝐗)=∑𝑘=1𝐾𝑝𝑘𝐏(𝐗|cluster 𝑘) 	 	 
where

 	 𝑝𝑘 	 = 	 𝐏(cluster 𝑘) 	 	 
 	 𝐗|cluster 𝑘 	 ∼ 	 (𝝁𝑘,Σ𝑘). 	 	 
Here,  𝐏(𝐗)  denotes the probability of obtaining the observation  𝐗 , and  𝐏(𝐗|cluster 𝑘)  is the probability of obtaining the observation  𝐗  given that it is generated by the model for cluster  𝑘 . This mixture has parameters  𝜃={𝑝1,…𝑝𝑘,𝝁1,…,𝝁𝐾,Σ1,…,Σ𝐾} . They correspond to the mixing proportions, means, and covariance matrices of each of the  𝐾  Gaussians.

Given  𝑛  data points  𝐱(1),…,𝐱(𝑛)  in  ℝ𝑑 , our goal is to set our parameters  𝜃  to maximize the data log-likelihood:

 	 ℓ(𝐱(1),…,𝐱(𝑛);𝜃) 	 = 	 log∏𝑖=1𝑛𝐏(𝐱(𝑖);𝜃)=∑𝑖=1𝑛log[∑𝑘=1𝐾𝑝𝑘𝐏(𝐱(𝑖)|cluster 𝑘;𝜃)]. 	 	 
There is no closed-form solution to finding the parameter set  𝜃  that maximizes this likelihood. The EM algorithm is an iterative algorithm that finds a locally optimal solution  𝜃̂   to the GMM likelihood maximization problem.

**E Step**

The E Step of the algorithm involves finding the posterior probability  𝑝(𝑘∣𝑖)=𝐏(cluster 𝑘|𝐱(𝑖);𝜃)  that point  𝐱(𝑖)  was generated by cluster  𝑘 , for every  𝑖=1,…,𝑛  and  𝑘=1,…,𝐾 . This step assumes the knowledge of the parameter set  𝜃 . We find the posterior using Bayes' rule:

 	 𝑝(𝑘∣𝑖)=𝐏(cluster 𝑘|𝐱(𝑖);𝜃) 	 = 	 𝑝𝑘𝐏(𝐱(𝑖)|cluster 𝑘;𝜃)𝐏(𝐱(𝑖);𝜃)=𝑝𝑘(𝐱(𝑖);𝜇(𝑘),Σ𝑘)∑𝐾𝑗=1𝑝𝑘(𝐱(𝑖);𝜇(𝑗),Σ𝑗) 	 	 
**M Step**

The M Step of the algorithm maximizes the expected log likelihood function  ℓ̃ (𝐱(1),…,𝐱(𝑛)𝜃) , which is a lower bound on the log-likelihood. The algorithm thus iteratively pushes the data likelihood upwards.

The expected log likelihood function  ℓ̃ (𝐱(1),…,𝐱(𝑛)𝜃)  is

 	 ℓ̃ (𝐱(1),…,𝐱(𝑛);𝜃) 	 = 	 ∑𝑖=1𝑛[∑𝑘=1𝐾𝑝(𝑘∣𝑖)log(𝐏(𝐱(𝑖),cluster 𝑘;𝜃)𝑝(𝑘∣𝑖))] 	 	 
 	 	 = 	 ∑𝑖=1𝑛[∑𝑘=1𝐾𝑝(𝑘∣𝑖)log(𝑝𝑘(𝐱(𝑖);𝜇(𝑘),Σ𝑘)𝑝(𝑘∣𝑖))] 	 	 
where recall the short hand notation for the posterior distribution is  𝑝(𝑘∣𝑖)=𝐏(cluster 𝑘|𝐱(𝑖);𝜃) .

This expected log likelihood function is a lower bound on the actual log-likelihood

 	 ℓ(𝐱(1),…,𝐱(𝑛);𝜃)=∑𝑖=1𝑛log[∑𝑘=1𝐾𝐏(𝐱(𝑖),cluster 𝑘;𝜃)]. 	 	 
(This is due to Jensen's inequality.)

In the special case where the covariance matrix pf the  𝑘 -th Gaussian is  Σ𝑘=𝜎2𝑘𝐈 , the parameters that maximize the above expected log likelihood function are as follows.

 	 𝝁(𝑘)ˆ 	 =∑𝑛𝑖=1𝐱(𝑖)𝑝(𝑘∣𝑖)∑𝑛𝑖=1𝑝(𝑘∣𝑖) 	 	 
 	 𝑝𝑘ˆ 	 =1𝑛∑𝑖=1𝑛𝑝(𝑘∣𝑖), 	 	 
 	 𝜎2𝑘ˆ 	 =∑𝑛𝑖=1𝑝(𝑘∣𝑖)‖𝐱(𝑖)−𝜇(𝑘)ˆ‖2𝑑∑𝑛𝑖=1𝑝(𝑘∣𝑖) 	 	 
You can verify this by taking derivatives and setting them equal to zero.

The E and M steps are repeated iteratively until there is no noticeable change in the actual likelihood computed after M step using the newly estimated parameters or if the parameters do not vary by much.

**Initialization**

for the initialization before the first time E step is carried out, we can either do a random initialization of the parameter set  𝜃  or we can employ K-means to find the initial cluster centers of the  𝐾  clusters and use the global variance of the dataset as the initial variance of all the  𝐾  clusters. In the latter case, the mixture weights can be initialized to the proportion of data points in the clusters as found by the k-means algorithm.


## Hierarchical Clustering

Hierarchical clustering does not start with a fixed chosen number of clusters, but builds a hierarchy of clusters with different levels corresponding to different numbers of clusters.

We can use a bottom-up or top-down approach:

- Agglomerative clustering (Bottom-Up)

- Divisive clustering (Top-down)

We will only discuss the bottom-up approach.

**Agglomerative Clustering (Bottom-Up approach)**

**Agglomerative clustering** starts with 1 data point per cluster, and at each consequent stage, merges pairs of clusters that are the closest together according to a dissimilarity measure between clusters.

This merging can be depicted by a tree, also known as a dendrogram . The bottom-most level has  𝑛  clusters (of 1 observation each), and as merging occurs as the levels go up, the number of clusters decreases, and the top-most level has only  1  cluster (encompassing all observations). See the recitation in this module for an example of hierarchical clustering and the associated dendrogram.

**Dissimilarity between clusters**

In order to choose which pair of clusters to merge at each stage, we need to define a dissimilarity measure between clusters, and the dissimilarity measure between clusters is often based on dissimilarity between points. A few commonly used distances between individual points are:

- 𝑙2− norm, i.e. the usual Euclidean distance

 	 𝑑(𝐱(𝑖),𝐱(𝑗))=(𝑥(𝑖)1−𝑥(𝑗)1)2+(𝑥(𝑖)2−𝑥(𝑗)2)2+⋯+(𝑥(𝑖)𝑝−𝑥(𝑗)𝑝)2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√(𝐱(𝑖)∈ℝ𝑝) 	 	 
- 𝑙1− norm (also known as Manhattan distance)

 	 𝑑(𝐱(𝑖),𝐱(𝑗))=∣∣𝑥(𝑖)1−𝑥(𝑗)1∣∣+∣∣𝑥(𝑖)2−𝑥(𝑗)2∣∣+⋯+∣∣𝑥(𝑖)𝑝−𝑥(𝑗)𝑝∣∣(𝐱(𝑖)∈ℝ𝑝) 	 	 
- 𝜆∞− norm, i.e. the maximum distance among all coordinates

 	 𝑑(𝐱(𝑖),𝐱(𝑗))=max𝑘=1,…,𝑝∣∣𝑥(𝑖)𝑘−𝑥(𝑗)𝑘∣∣(𝐱(𝑖)∈ℝ𝑝) 	 	 
- Other dissimilarity measures  𝑑(𝐱(𝑖),𝐱(𝑗))  that do not satisfy all properties of distances, but still meet the following criteria:

 	 𝑑(𝐱(𝑖),𝐱(𝑗)) 	 ≥ 	 0(positivity) 	 	 
 	 𝑑(𝐱(𝑖),𝐱(𝑖)) 	 = 	 0 	 	 
 	 𝑑(𝐱(𝑖),𝐱(𝑗)) 	 = 	 𝑑(𝐱(𝑗),𝐱(𝑖))(symmetry) 	 	 
Now we can define dissimalarity measures between clusters:

- **Minimum distance** between points in the two clusters, also known as single linkage :

 	 𝑑(𝐶1,𝐶2) 	 = 	 min𝐱(𝑖)∈𝐶1,𝐱(𝑗)∈𝐶2𝑑(𝐱(𝑖),𝐱(𝑗)). 	 	 
- **Maximum distance** between points in the two clusters, also known as complete linkage :

 	 𝑑(𝐶1,𝐶2) 	 = 	 max𝐱(𝑖)∈𝐶1,𝐱(𝑗)∈𝐶2𝑑(𝐱(𝑖),𝐱(𝑗)). 	 	 
- **Average distance** between points in the two different clusters, also known as average linkage

 	 𝑑(𝐶1,𝐶2) 	 = 	 1𝑛1𝑛2∑𝐱(𝑖)∈𝐶1∑𝐱(𝑗)∈𝐶2𝑑(𝐱(𝑖),𝐱(𝑗)). 	 



## DBSCAN

Density-based spatial clustering of applications with noise (DBSCAN) aims to cluster together points that are close to each another in a dense region, and leave out points that are in low density regions.

To perform DBSCAN, we need to choose two parameters:

- 𝜖 , distance between connected points.

- 𝑘 , core strength

Let two points be connected if they are within a distance  𝜖  of one another. A core point is a point that are connected to at least  𝑘  other points.

Then two points are placed into the same cluster if and only if there is a connecting path between them consisting of only core points, except possibly at the ends of the path.


In the figure above, the blue points are core points for core strength  𝑘=4 , since they are each connected to at least 4 other points. Two clusters are formed. In each cluster, each non-core (black) point is connected to a core point (blue) . The points that are not connected to any core points are outliers and not clustered into any cluster in DBSCAN.
